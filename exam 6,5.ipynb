{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3c4caf",
   "metadata": {},
   "source": [
    "News S&P 500 next-day direction\n",
    "\n",
    "This builds a daily dataset from a CSV of headlines + close prices, then trains a small baseline classifier to predict whether the next day closes higher than the current day.\n",
    "\n",
    "Two feature modes:\n",
    "- bag-of-words: fast, shallow baseline on the concatenated daily headlines.\n",
    "- FinBERT: uses a pretrained sentiment model to produce daily sentiment probabilities (neg/neu/pos) + headline count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190246a",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807db816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd804815",
   "metadata": {},
   "source": [
    "Hashed bag-of-words BoW\n",
    "\n",
    "We tokenize by extracting only letters (a–z, A–Z), lowercasing, then counting tokens into hash bins.\n",
    "we use `log1p` to reduce the impact of frequent words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c72cb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_re = re.compile(r\"[a-zA-Z]+\")\n",
    "\n",
    "def toks(s):\n",
    "    # Return list of lowercase word tokens from string\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return word_re.findall(s.lower())\n",
    "\n",
    "def stable_hash(word, bins):\n",
    "    #Using md5 so it is stable across runs\n",
    "    h = hashlib.md5(word.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h[:8], 16) % bins\n",
    "\n",
    "def make_hash_X(texts, bins=50000):\n",
    "    # Build hashed BoW features for a list of texts\n",
    "    X = np.zeros((len(texts), bins), dtype=np.float32)\n",
    "    for i, t in enumerate(texts):\n",
    "        for w in toks(t):\n",
    "            X[i, stable_hash(w, bins)] += 1.0\n",
    "    X = np.log1p(X)\n",
    "    return torch.tensor(X, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7005f4e",
   "metadata": {},
   "source": [
    "Training loop, logistic regression\n",
    "\n",
    "We treat the task as binary classification:\n",
    "y = 1 if close_next > close\n",
    "y = 0 otherwise\n",
    "\n",
    "Model is very simple, it is one linear layer outputting a single logit.\n",
    "Loss is defined as BCEWithLogitsLoss expects logits directly\n",
    "Note, remmeber to delete print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab16a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(model, X, y):\n",
    "    # Compute accuracy for a given model and data.\n",
    "    dev = next(model.parameters()).device\n",
    "    X, y = X.to(dev), y.to(dev)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        p = torch.sigmoid(model(X).squeeze(1))\n",
    "        pred = (p >= 0.5).float()\n",
    "        return (pred == y).float().mean().item()\n",
    "\n",
    "def train_lr(Xtr, ytr, Xva, yva, epochs=15, lr=0.2, eval_every=1):\n",
    "    #Train a logistic-regression style linear model\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    Xtr, ytr = Xtr.to(dev), ytr.to(dev)\n",
    "    Xva, yva = Xva.to(dev), yva.to(dev)\n",
    "\n",
    "    model = torch.nn.Linear(Xtr.shape[1], 1).to(dev)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits = model(Xtr).squeeze(1)\n",
    "        loss = loss_fn(logits, ytr)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if eval_every and (ep % eval_every == 0):\n",
    "            va_acc = eval_acc(model, Xva, yva)\n",
    "            print(f\"ep {ep:02d} | loss {loss.item():.4f} | val_acc {va_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"ep {ep:02d} | loss {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def majority_baseline_acc(y_train, y_test):\n",
    "    # Baseline: always predict the most common class in training\n",
    "    p = 1.0 if np.mean(y_train) >= 0.5 else 0.0\n",
    "    pred = np.full_like(y_test, p, dtype=float)\n",
    "    acc = (pred == y_test).mean()\n",
    "    print(f\"majority_baseline_acc {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ffd5d",
   "metadata": {},
   "source": [
    "FinBERT daily sentiment features, also caching.\n",
    "\n",
    "For each day, run FinBERT on each headline then convert logits to probabilities with softmax (neg/neu/pos).\n",
    "we then averege probabilities across headlines to get daily sentiment vector,\n",
    "Add n_headlines as an extra feature as number of headlines. \n",
    "\n",
    "FinBERT is somehow slow. We cache the daily features to a CSV so repeated runs are fast. (it is fast running ones, but when testing 100 of times this is nice to have)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de86319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_feats(day_to_titles, model_name, bs=16, max_len=64):\n",
    "    dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokz = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(model_name).to(dev)\n",
    "    mdl.eval()\n",
    "\n",
    "    dates = []\n",
    "    feats = []\n",
    "    n_news = []\n",
    "\n",
    "    for d, titles in day_to_titles.items():\n",
    "        titles = [t for t in titles if isinstance(t, str) and t.strip() != \"\"]\n",
    "        n_news.append(len(titles))\n",
    "\n",
    "        if len(titles) == 0:\n",
    "            # if a day has no headlines, set neutral\n",
    "            dates.append(d)\n",
    "            feats.append([0.0, 1.0, 0.0])\n",
    "            continue\n",
    "\n",
    "        probs_all = []\n",
    "        for i in range(0, len(titles), bs):\n",
    "            batch = titles[i:i + bs]\n",
    "            enc = tokz(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(dev) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                out = mdl(**enc)\n",
    "                probs = torch.softmax(out.logits, dim=1)  # neg/neu/pos\n",
    "                probs_all.append(probs.cpu())\n",
    "\n",
    "        probs_all = torch.cat(probs_all, dim=0)\n",
    "        feats.append(probs_all.mean(dim=0).tolist())\n",
    "        dates.append(d)\n",
    "\n",
    "    f = pd.DataFrame(feats, columns=[\"p_neg\", \"p_neu\", \"p_pos\"])\n",
    "    f[\"date\"] = dates\n",
    "    f[\"n_headlines\"] = n_news\n",
    "    return f\n",
    "\n",
    "def get_finbert_daily(day_df, model_name, bs=16, max_len=64):\n",
    "    safe = model_name.replace(\"/\", \"_\")\n",
    "    cache_path = f\"finbert_daily_cache_{safe}_len{max_len}.csv\"\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        f = pd.read_csv(cache_path)\n",
    "        f[\"date\"] = pd.to_datetime(f[\"date\"]).dt.date\n",
    "        return f\n",
    "\n",
    "    day_map = dict(zip(day_df[\"date\"], day_df[\"title_list\"]))\n",
    "    f = finbert_feats(day_map, model_name, bs=bs, max_len=max_len)\n",
    "    f.to_csv(cache_path, index=False)\n",
    "    return f\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b520b",
   "metadata": {},
   "source": [
    "Load CSV and build dataset\n",
    "\n",
    "Expected columns in the CSV:\n",
    "- Date= date of the headline row\n",
    "- cp= close price (or the price column you want to use)\n",
    "- Title= headline text\n",
    "\n",
    "Steps: \n",
    "1. Parse data\n",
    "2. Aggregate by day (one groupby pass):\n",
    "   - text: concatenated headlines (for BoW)\n",
    "   - title_list: list of headlines (for FinBERT)\n",
    "   - close: last close of that day\n",
    "3. Create target y using the next day close\n",
    "4. Split by year into train/val/test (time-based split)\n",
    "\n",
    "OBS set bow of finbert manualy!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8f22b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_baseline_acc 0.4953\n",
      "rows: 2222 747 537\n"
     ]
    }
   ],
   "source": [
    "CSV_PATH = \"sp500_headlines_2008_2024.csv\"\n",
    "MODE = \"finbert\"  # \"bow\" or finbert\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.date\n",
    "df[\"close\"] = pd.to_numeric(df[\"CP\"], errors=\"coerce\")\n",
    "df[\"title\"] = df[\"Title\"].astype(str)\n",
    "df = df.dropna(subset=[\"date\", \"close\", \"title\"]).copy()\n",
    "\n",
    "# aggregate per day \n",
    "day = (\n",
    "    df.sort_values(\"date\")\n",
    "      .groupby(\"date\", as_index=False)\n",
    "      .agg(\n",
    "          text=(\"title\", lambda s: \" . \".join(s.tolist())),\n",
    "          title_list=(\"title\", lambda s: s.tolist()),\n",
    "          close=(\"close\", \"last\"),\n",
    "      )\n",
    "      .sort_values(\"date\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# next day direction label\n",
    "day[\"close_next\"] = day[\"close\"].shift(-1)\n",
    "day = day.dropna(subset=[\"close_next\"]).copy()\n",
    "day[\"y\"] = (day[\"close_next\"] > day[\"close\"]).astype(np.float32)\n",
    "\n",
    "day[\"year\"] = pd.to_datetime(day[\"date\"]).dt.year\n",
    "\n",
    "train = day[day[\"year\"] <= 2018].copy()\n",
    "val   = day[(day[\"year\"] >= 2019) & (day[\"year\"] <= 2021)].copy()\n",
    "test  = day[day[\"year\"] >= 2022].copy()\n",
    "\n",
    "# baseline just to know if the model is doing anything useful\n",
    "majority_baseline_acc(train[\"y\"].values, test[\"y\"].values)\n",
    "\n",
    "ytr = torch.tensor(train[\"y\"].values, dtype=torch.float32)\n",
    "yva = torch.tensor(val[\"y\"].values, dtype=torch.float32)\n",
    "yte = torch.tensor(test[\"y\"].values, dtype=torch.float32)\n",
    "\n",
    "print(\"rows:\", len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16e224",
   "metadata": {},
   "source": [
    "Train depending on feature mode from earlier \n",
    "\n",
    "At the end we see test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d7a0cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | loss 0.8361\n",
      "ep 02 | loss 0.7046 | val_acc 0.5529\n",
      "ep 03 | loss 0.7000\n",
      "ep 04 | loss 0.6982 | val_acc 0.5502\n",
      "ep 05 | loss 0.6973\n",
      "ep 06 | loss 0.6967 | val_acc 0.5515\n",
      "ep 07 | loss 0.6962\n",
      "ep 08 | loss 0.6958 | val_acc 0.5582\n",
      "ep 09 | loss 0.6954\n",
      "ep 10 | loss 0.6950 | val_acc 0.5676\n",
      "ep 11 | loss 0.6947\n",
      "ep 12 | loss 0.6944 | val_acc 0.5689\n",
      "ep 13 | loss 0.6941\n",
      "ep 14 | loss 0.6938 | val_acc 0.5689\n",
      "ep 15 | loss 0.6936\n",
      "ep 16 | loss 0.6933 | val_acc 0.5703\n",
      "ep 17 | loss 0.6931\n",
      "ep 18 | loss 0.6929 | val_acc 0.5689\n",
      "ep 19 | loss 0.6927\n",
      "ep 20 | loss 0.6925 | val_acc 0.5730\n",
      "ep 21 | loss 0.6924\n",
      "ep 22 | loss 0.6922 | val_acc 0.5730\n",
      "ep 23 | loss 0.6921\n",
      "ep 24 | loss 0.6920 | val_acc 0.5743\n",
      "ep 25 | loss 0.6918\n",
      "ep 26 | loss 0.6917 | val_acc 0.5743\n",
      "ep 27 | loss 0.6916\n",
      "ep 28 | loss 0.6915 | val_acc 0.5743\n",
      "ep 29 | loss 0.6914\n",
      "ep 30 | loss 0.6913 | val_acc 0.5730\n",
      "ep 31 | loss 0.6913\n",
      "ep 32 | loss 0.6912 | val_acc 0.5743\n",
      "ep 33 | loss 0.6911\n",
      "ep 34 | loss 0.6910 | val_acc 0.5770\n",
      "ep 35 | loss 0.6910\n",
      "ep 36 | loss 0.6909 | val_acc 0.5756\n",
      "ep 37 | loss 0.6909\n",
      "ep 38 | loss 0.6908 | val_acc 0.5756\n",
      "ep 39 | loss 0.6907\n",
      "ep 40 | loss 0.6907 | val_acc 0.5743\n",
      "ep 41 | loss 0.6907\n",
      "ep 42 | loss 0.6906 | val_acc 0.5743\n",
      "ep 43 | loss 0.6906\n",
      "ep 44 | loss 0.6905 | val_acc 0.5716\n",
      "ep 45 | loss 0.6905\n",
      "ep 46 | loss 0.6905 | val_acc 0.5743\n",
      "ep 47 | loss 0.6904\n",
      "ep 48 | loss 0.6904 | val_acc 0.5730\n",
      "ep 49 | loss 0.6904\n",
      "ep 50 | loss 0.6903 | val_acc 0.5730\n",
      "ep 51 | loss 0.6903\n",
      "ep 52 | loss 0.6903 | val_acc 0.5716\n",
      "ep 53 | loss 0.6903\n",
      "ep 54 | loss 0.6902 | val_acc 0.5716\n",
      "ep 55 | loss 0.6902\n",
      "ep 56 | loss 0.6902 | val_acc 0.5703\n",
      "ep 57 | loss 0.6902\n",
      "ep 58 | loss 0.6901 | val_acc 0.5703\n",
      "ep 59 | loss 0.6901\n",
      "ep 60 | loss 0.6901 | val_acc 0.5703\n",
      "ep 61 | loss 0.6901\n",
      "ep 62 | loss 0.6901 | val_acc 0.5689\n",
      "ep 63 | loss 0.6900\n",
      "ep 64 | loss 0.6900 | val_acc 0.5676\n",
      "ep 65 | loss 0.6900\n",
      "ep 66 | loss 0.6900 | val_acc 0.5689\n",
      "ep 67 | loss 0.6900\n",
      "ep 68 | loss 0.6900 | val_acc 0.5689\n",
      "ep 69 | loss 0.6899\n",
      "ep 70 | loss 0.6899 | val_acc 0.5689\n",
      "ep 71 | loss 0.6899\n",
      "ep 72 | loss 0.6899 | val_acc 0.5703\n",
      "ep 73 | loss 0.6899\n",
      "ep 74 | loss 0.6899 | val_acc 0.5703\n",
      "ep 75 | loss 0.6899\n",
      "ep 76 | loss 0.6899 | val_acc 0.5703\n",
      "ep 77 | loss 0.6898\n",
      "ep 78 | loss 0.6898 | val_acc 0.5703\n",
      "ep 79 | loss 0.6898\n",
      "ep 80 | loss 0.6898 | val_acc 0.5703\n",
      "ep 81 | loss 0.6898\n",
      "ep 82 | loss 0.6898 | val_acc 0.5703\n",
      "ep 83 | loss 0.6898\n",
      "ep 84 | loss 0.6898 | val_acc 0.5703\n",
      "ep 85 | loss 0.6898\n",
      "ep 86 | loss 0.6898 | val_acc 0.5703\n",
      "ep 87 | loss 0.6897\n",
      "ep 88 | loss 0.6897 | val_acc 0.5703\n",
      "ep 89 | loss 0.6897\n",
      "ep 90 | loss 0.6897 | val_acc 0.5703\n",
      "ep 91 | loss 0.6897\n",
      "ep 92 | loss 0.6897 | val_acc 0.5730\n",
      "ep 93 | loss 0.6897\n",
      "ep 94 | loss 0.6897 | val_acc 0.5730\n",
      "ep 95 | loss 0.6897\n",
      "ep 96 | loss 0.6897 | val_acc 0.5730\n",
      "ep 97 | loss 0.6897\n",
      "ep 98 | loss 0.6897 | val_acc 0.5730\n",
      "ep 99 | loss 0.6897\n",
      "ep 100 | loss 0.6897 | val_acc 0.5743\n",
      "ep 101 | loss 0.6896\n",
      "ep 102 | loss 0.6896 | val_acc 0.5756\n",
      "ep 103 | loss 0.6896\n",
      "ep 104 | loss 0.6896 | val_acc 0.5770\n",
      "ep 105 | loss 0.6896\n",
      "ep 106 | loss 0.6896 | val_acc 0.5770\n",
      "ep 107 | loss 0.6896\n",
      "ep 108 | loss 0.6896 | val_acc 0.5770\n",
      "ep 109 | loss 0.6896\n",
      "ep 110 | loss 0.6896 | val_acc 0.5770\n",
      "ep 111 | loss 0.6896\n",
      "ep 112 | loss 0.6896 | val_acc 0.5770\n",
      "ep 113 | loss 0.6896\n",
      "ep 114 | loss 0.6896 | val_acc 0.5770\n",
      "ep 115 | loss 0.6896\n",
      "ep 116 | loss 0.6896 | val_acc 0.5770\n",
      "ep 117 | loss 0.6896\n",
      "ep 118 | loss 0.6896 | val_acc 0.5770\n",
      "ep 119 | loss 0.6896\n",
      "ep 120 | loss 0.6896 | val_acc 0.5770\n",
      "ep 121 | loss 0.6895\n",
      "ep 122 | loss 0.6895 | val_acc 0.5770\n",
      "ep 123 | loss 0.6895\n",
      "ep 124 | loss 0.6895 | val_acc 0.5770\n",
      "ep 125 | loss 0.6895\n",
      "ep 126 | loss 0.6895 | val_acc 0.5770\n",
      "ep 127 | loss 0.6895\n",
      "ep 128 | loss 0.6895 | val_acc 0.5770\n",
      "ep 129 | loss 0.6895\n",
      "ep 130 | loss 0.6895 | val_acc 0.5770\n",
      "ep 131 | loss 0.6895\n",
      "ep 132 | loss 0.6895 | val_acc 0.5770\n",
      "ep 133 | loss 0.6895\n",
      "ep 134 | loss 0.6895 | val_acc 0.5770\n",
      "ep 135 | loss 0.6895\n",
      "ep 136 | loss 0.6895 | val_acc 0.5770\n",
      "ep 137 | loss 0.6895\n",
      "ep 138 | loss 0.6895 | val_acc 0.5756\n",
      "ep 139 | loss 0.6895\n",
      "ep 140 | loss 0.6895 | val_acc 0.5756\n",
      "ep 141 | loss 0.6895\n",
      "ep 142 | loss 0.6895 | val_acc 0.5756\n",
      "ep 143 | loss 0.6895\n",
      "ep 144 | loss 0.6895 | val_acc 0.5756\n",
      "ep 145 | loss 0.6895\n",
      "ep 146 | loss 0.6895 | val_acc 0.5756\n",
      "ep 147 | loss 0.6895\n",
      "ep 148 | loss 0.6895 | val_acc 0.5756\n",
      "ep 149 | loss 0.6895\n",
      "ep 150 | loss 0.6895 | val_acc 0.5756\n",
      "ep 151 | loss 0.6895\n",
      "ep 152 | loss 0.6895 | val_acc 0.5756\n",
      "ep 153 | loss 0.6895\n",
      "ep 154 | loss 0.6895 | val_acc 0.5756\n",
      "ep 155 | loss 0.6895\n",
      "ep 156 | loss 0.6895 | val_acc 0.5756\n",
      "ep 157 | loss 0.6895\n",
      "ep 158 | loss 0.6895 | val_acc 0.5756\n",
      "ep 159 | loss 0.6895\n",
      "ep 160 | loss 0.6895 | val_acc 0.5756\n",
      "ep 161 | loss 0.6894\n",
      "ep 162 | loss 0.6894 | val_acc 0.5756\n",
      "ep 163 | loss 0.6894\n",
      "ep 164 | loss 0.6894 | val_acc 0.5756\n",
      "ep 165 | loss 0.6894\n",
      "ep 166 | loss 0.6894 | val_acc 0.5756\n",
      "ep 167 | loss 0.6894\n",
      "ep 168 | loss 0.6894 | val_acc 0.5756\n",
      "ep 169 | loss 0.6894\n",
      "ep 170 | loss 0.6894 | val_acc 0.5756\n",
      "ep 171 | loss 0.6894\n",
      "ep 172 | loss 0.6894 | val_acc 0.5756\n",
      "ep 173 | loss 0.6894\n",
      "ep 174 | loss 0.6894 | val_acc 0.5756\n",
      "ep 175 | loss 0.6894\n",
      "ep 176 | loss 0.6894 | val_acc 0.5756\n",
      "ep 177 | loss 0.6894\n",
      "ep 178 | loss 0.6894 | val_acc 0.5756\n",
      "ep 179 | loss 0.6894\n",
      "ep 180 | loss 0.6894 | val_acc 0.5756\n",
      "ep 181 | loss 0.6894\n",
      "ep 182 | loss 0.6894 | val_acc 0.5756\n",
      "ep 183 | loss 0.6894\n",
      "ep 184 | loss 0.6894 | val_acc 0.5756\n",
      "ep 185 | loss 0.6894\n",
      "ep 186 | loss 0.6894 | val_acc 0.5756\n",
      "ep 187 | loss 0.6894\n",
      "ep 188 | loss 0.6894 | val_acc 0.5756\n",
      "ep 189 | loss 0.6894\n",
      "ep 190 | loss 0.6894 | val_acc 0.5756\n",
      "ep 191 | loss 0.6894\n",
      "ep 192 | loss 0.6894 | val_acc 0.5756\n",
      "ep 193 | loss 0.6894\n",
      "ep 194 | loss 0.6894 | val_acc 0.5756\n",
      "ep 195 | loss 0.6894\n",
      "ep 196 | loss 0.6894 | val_acc 0.5756\n",
      "ep 197 | loss 0.6894\n",
      "ep 198 | loss 0.6894 | val_acc 0.5756\n",
      "ep 199 | loss 0.6894\n",
      "ep 200 | loss 0.6894 | val_acc 0.5756\n",
      "ep 201 | loss 0.6894\n",
      "ep 202 | loss 0.6894 | val_acc 0.5756\n",
      "ep 203 | loss 0.6894\n",
      "ep 204 | loss 0.6894 | val_acc 0.5756\n",
      "ep 205 | loss 0.6894\n",
      "ep 206 | loss 0.6894 | val_acc 0.5756\n",
      "ep 207 | loss 0.6894\n",
      "ep 208 | loss 0.6894 | val_acc 0.5756\n",
      "ep 209 | loss 0.6894\n",
      "ep 210 | loss 0.6894 | val_acc 0.5756\n",
      "ep 211 | loss 0.6894\n",
      "ep 212 | loss 0.6894 | val_acc 0.5756\n",
      "ep 213 | loss 0.6894\n",
      "ep 214 | loss 0.6894 | val_acc 0.5756\n",
      "ep 215 | loss 0.6894\n",
      "ep 216 | loss 0.6894 | val_acc 0.5756\n",
      "ep 217 | loss 0.6894\n",
      "ep 218 | loss 0.6894 | val_acc 0.5756\n",
      "ep 219 | loss 0.6894\n",
      "ep 220 | loss 0.6894 | val_acc 0.5756\n",
      "ep 221 | loss 0.6894\n",
      "ep 222 | loss 0.6894 | val_acc 0.5756\n",
      "ep 223 | loss 0.6894\n",
      "ep 224 | loss 0.6894 | val_acc 0.5756\n",
      "ep 225 | loss 0.6894\n",
      "ep 226 | loss 0.6894 | val_acc 0.5756\n",
      "ep 227 | loss 0.6894\n",
      "ep 228 | loss 0.6894 | val_acc 0.5756\n",
      "ep 229 | loss 0.6894\n",
      "ep 230 | loss 0.6894 | val_acc 0.5756\n",
      "ep 231 | loss 0.6894\n",
      "ep 232 | loss 0.6894 | val_acc 0.5756\n",
      "ep 233 | loss 0.6894\n",
      "ep 234 | loss 0.6894 | val_acc 0.5756\n",
      "ep 235 | loss 0.6894\n",
      "ep 236 | loss 0.6894 | val_acc 0.5756\n",
      "ep 237 | loss 0.6894\n",
      "ep 238 | loss 0.6894 | val_acc 0.5756\n",
      "ep 239 | loss 0.6894\n",
      "ep 240 | loss 0.6894 | val_acc 0.5756\n",
      "ep 241 | loss 0.6894\n",
      "ep 242 | loss 0.6894 | val_acc 0.5756\n",
      "ep 243 | loss 0.6894\n",
      "ep 244 | loss 0.6894 | val_acc 0.5756\n",
      "ep 245 | loss 0.6894\n",
      "ep 246 | loss 0.6894 | val_acc 0.5756\n",
      "ep 247 | loss 0.6894\n",
      "ep 248 | loss 0.6894 | val_acc 0.5756\n",
      "ep 249 | loss 0.6894\n",
      "ep 250 | loss 0.6894 | val_acc 0.5756\n",
      "ep 251 | loss 0.6894\n",
      "ep 252 | loss 0.6894 | val_acc 0.5756\n",
      "ep 253 | loss 0.6894\n",
      "ep 254 | loss 0.6894 | val_acc 0.5756\n",
      "ep 255 | loss 0.6894\n",
      "ep 256 | loss 0.6894 | val_acc 0.5756\n",
      "ep 257 | loss 0.6894\n",
      "ep 258 | loss 0.6894 | val_acc 0.5756\n",
      "ep 259 | loss 0.6894\n",
      "ep 260 | loss 0.6894 | val_acc 0.5756\n",
      "ep 261 | loss 0.6894\n",
      "ep 262 | loss 0.6894 | val_acc 0.5756\n",
      "ep 263 | loss 0.6894\n",
      "ep 264 | loss 0.6894 | val_acc 0.5756\n",
      "ep 265 | loss 0.6894\n",
      "ep 266 | loss 0.6894 | val_acc 0.5756\n",
      "ep 267 | loss 0.6894\n",
      "ep 268 | loss 0.6894 | val_acc 0.5756\n",
      "ep 269 | loss 0.6894\n",
      "ep 270 | loss 0.6894 | val_acc 0.5756\n",
      "ep 271 | loss 0.6894\n",
      "ep 272 | loss 0.6894 | val_acc 0.5756\n",
      "ep 273 | loss 0.6894\n",
      "ep 274 | loss 0.6894 | val_acc 0.5756\n",
      "ep 275 | loss 0.6894\n",
      "ep 276 | loss 0.6894 | val_acc 0.5756\n",
      "ep 277 | loss 0.6894\n",
      "ep 278 | loss 0.6894 | val_acc 0.5756\n",
      "ep 279 | loss 0.6894\n",
      "ep 280 | loss 0.6894 | val_acc 0.5756\n",
      "ep 281 | loss 0.6894\n",
      "ep 282 | loss 0.6894 | val_acc 0.5756\n",
      "ep 283 | loss 0.6894\n",
      "ep 284 | loss 0.6894 | val_acc 0.5756\n",
      "ep 285 | loss 0.6894\n",
      "ep 286 | loss 0.6894 | val_acc 0.5756\n",
      "ep 287 | loss 0.6894\n",
      "ep 288 | loss 0.6894 | val_acc 0.5756\n",
      "ep 289 | loss 0.6894\n",
      "ep 290 | loss 0.6894 | val_acc 0.5756\n",
      "ep 291 | loss 0.6894\n",
      "ep 292 | loss 0.6894 | val_acc 0.5756\n",
      "ep 293 | loss 0.6894\n",
      "ep 294 | loss 0.6894 | val_acc 0.5756\n",
      "ep 295 | loss 0.6894\n",
      "ep 296 | loss 0.6894 | val_acc 0.5756\n",
      "ep 297 | loss 0.6894\n",
      "ep 298 | loss 0.6894 | val_acc 0.5756\n",
      "ep 299 | loss 0.6894\n",
      "ep 300 | loss 0.6894 | val_acc 0.5756\n",
      "ep 301 | loss 0.6894\n",
      "ep 302 | loss 0.6894 | val_acc 0.5756\n",
      "ep 303 | loss 0.6894\n",
      "ep 304 | loss 0.6894 | val_acc 0.5756\n",
      "ep 305 | loss 0.6894\n",
      "ep 306 | loss 0.6894 | val_acc 0.5756\n",
      "ep 307 | loss 0.6894\n",
      "ep 308 | loss 0.6894 | val_acc 0.5756\n",
      "ep 309 | loss 0.6894\n",
      "ep 310 | loss 0.6894 | val_acc 0.5756\n",
      "ep 311 | loss 0.6894\n",
      "ep 312 | loss 0.6894 | val_acc 0.5756\n",
      "ep 313 | loss 0.6894\n",
      "ep 314 | loss 0.6894 | val_acc 0.5756\n",
      "ep 315 | loss 0.6894\n",
      "ep 316 | loss 0.6894 | val_acc 0.5756\n",
      "ep 317 | loss 0.6894\n",
      "ep 318 | loss 0.6894 | val_acc 0.5756\n",
      "ep 319 | loss 0.6894\n",
      "ep 320 | loss 0.6894 | val_acc 0.5756\n",
      "ep 321 | loss 0.6894\n",
      "ep 322 | loss 0.6894 | val_acc 0.5756\n",
      "ep 323 | loss 0.6894\n",
      "ep 324 | loss 0.6894 | val_acc 0.5756\n",
      "ep 325 | loss 0.6894\n",
      "ep 326 | loss 0.6894 | val_acc 0.5756\n",
      "ep 327 | loss 0.6894\n",
      "ep 328 | loss 0.6894 | val_acc 0.5756\n",
      "ep 329 | loss 0.6894\n",
      "ep 330 | loss 0.6894 | val_acc 0.5756\n",
      "ep 331 | loss 0.6894\n",
      "ep 332 | loss 0.6894 | val_acc 0.5756\n",
      "ep 333 | loss 0.6894\n",
      "ep 334 | loss 0.6894 | val_acc 0.5756\n",
      "ep 335 | loss 0.6894\n",
      "ep 336 | loss 0.6894 | val_acc 0.5756\n",
      "ep 337 | loss 0.6894\n",
      "ep 338 | loss 0.6894 | val_acc 0.5756\n",
      "ep 339 | loss 0.6894\n",
      "ep 340 | loss 0.6894 | val_acc 0.5756\n",
      "ep 341 | loss 0.6894\n",
      "ep 342 | loss 0.6894 | val_acc 0.5756\n",
      "ep 343 | loss 0.6894\n",
      "ep 344 | loss 0.6894 | val_acc 0.5756\n",
      "ep 345 | loss 0.6894\n",
      "ep 346 | loss 0.6894 | val_acc 0.5756\n",
      "ep 347 | loss 0.6894\n",
      "ep 348 | loss 0.6894 | val_acc 0.5756\n",
      "ep 349 | loss 0.6894\n",
      "ep 350 | loss 0.6894 | val_acc 0.5756\n",
      "ep 351 | loss 0.6894\n",
      "ep 352 | loss 0.6894 | val_acc 0.5756\n",
      "ep 353 | loss 0.6894\n",
      "ep 354 | loss 0.6894 | val_acc 0.5756\n",
      "ep 355 | loss 0.6894\n",
      "ep 356 | loss 0.6894 | val_acc 0.5756\n",
      "ep 357 | loss 0.6894\n",
      "ep 358 | loss 0.6894 | val_acc 0.5756\n",
      "ep 359 | loss 0.6894\n",
      "ep 360 | loss 0.6894 | val_acc 0.5756\n",
      "ep 361 | loss 0.6894\n",
      "ep 362 | loss 0.6894 | val_acc 0.5756\n",
      "ep 363 | loss 0.6894\n",
      "ep 364 | loss 0.6894 | val_acc 0.5756\n",
      "ep 365 | loss 0.6894\n",
      "ep 366 | loss 0.6894 | val_acc 0.5756\n",
      "ep 367 | loss 0.6894\n",
      "ep 368 | loss 0.6894 | val_acc 0.5756\n",
      "ep 369 | loss 0.6894\n",
      "ep 370 | loss 0.6894 | val_acc 0.5756\n",
      "ep 371 | loss 0.6894\n",
      "ep 372 | loss 0.6894 | val_acc 0.5756\n",
      "ep 373 | loss 0.6894\n",
      "ep 374 | loss 0.6894 | val_acc 0.5756\n",
      "ep 375 | loss 0.6894\n",
      "ep 376 | loss 0.6894 | val_acc 0.5756\n",
      "ep 377 | loss 0.6894\n",
      "ep 378 | loss 0.6894 | val_acc 0.5756\n",
      "ep 379 | loss 0.6894\n",
      "ep 380 | loss 0.6894 | val_acc 0.5756\n",
      "ep 381 | loss 0.6894\n",
      "ep 382 | loss 0.6894 | val_acc 0.5756\n",
      "ep 383 | loss 0.6894\n",
      "ep 384 | loss 0.6894 | val_acc 0.5756\n",
      "ep 385 | loss 0.6894\n",
      "ep 386 | loss 0.6894 | val_acc 0.5756\n",
      "ep 387 | loss 0.6894\n",
      "ep 388 | loss 0.6894 | val_acc 0.5756\n",
      "ep 389 | loss 0.6894\n",
      "ep 390 | loss 0.6894 | val_acc 0.5756\n",
      "ep 391 | loss 0.6894\n",
      "ep 392 | loss 0.6894 | val_acc 0.5756\n",
      "ep 393 | loss 0.6894\n",
      "ep 394 | loss 0.6894 | val_acc 0.5756\n",
      "ep 395 | loss 0.6894\n",
      "ep 396 | loss 0.6894 | val_acc 0.5756\n",
      "ep 397 | loss 0.6894\n",
      "ep 398 | loss 0.6894 | val_acc 0.5756\n",
      "ep 399 | loss 0.6894\n",
      "ep 400 | loss 0.6894 | val_acc 0.5756\n",
      "ep 401 | loss 0.6894\n",
      "ep 402 | loss 0.6894 | val_acc 0.5756\n",
      "ep 403 | loss 0.6894\n",
      "ep 404 | loss 0.6894 | val_acc 0.5756\n",
      "ep 405 | loss 0.6894\n",
      "ep 406 | loss 0.6894 | val_acc 0.5756\n",
      "ep 407 | loss 0.6894\n",
      "ep 408 | loss 0.6894 | val_acc 0.5756\n",
      "ep 409 | loss 0.6894\n",
      "ep 410 | loss 0.6894 | val_acc 0.5756\n",
      "ep 411 | loss 0.6894\n",
      "ep 412 | loss 0.6894 | val_acc 0.5756\n",
      "ep 413 | loss 0.6894\n",
      "ep 414 | loss 0.6894 | val_acc 0.5756\n",
      "ep 415 | loss 0.6894\n",
      "ep 416 | loss 0.6894 | val_acc 0.5756\n",
      "ep 417 | loss 0.6894\n",
      "ep 418 | loss 0.6894 | val_acc 0.5756\n",
      "ep 419 | loss 0.6894\n",
      "ep 420 | loss 0.6894 | val_acc 0.5756\n",
      "ep 421 | loss 0.6894\n",
      "ep 422 | loss 0.6894 | val_acc 0.5756\n",
      "ep 423 | loss 0.6894\n",
      "ep 424 | loss 0.6894 | val_acc 0.5756\n",
      "ep 425 | loss 0.6894\n",
      "ep 426 | loss 0.6894 | val_acc 0.5756\n",
      "ep 427 | loss 0.6894\n",
      "ep 428 | loss 0.6894 | val_acc 0.5756\n",
      "ep 429 | loss 0.6894\n",
      "ep 430 | loss 0.6894 | val_acc 0.5756\n",
      "ep 431 | loss 0.6894\n",
      "ep 432 | loss 0.6894 | val_acc 0.5756\n",
      "ep 433 | loss 0.6894\n",
      "ep 434 | loss 0.6894 | val_acc 0.5756\n",
      "ep 435 | loss 0.6894\n",
      "ep 436 | loss 0.6894 | val_acc 0.5756\n",
      "ep 437 | loss 0.6894\n",
      "ep 438 | loss 0.6894 | val_acc 0.5756\n",
      "ep 439 | loss 0.6894\n",
      "ep 440 | loss 0.6894 | val_acc 0.5756\n",
      "ep 441 | loss 0.6894\n",
      "ep 442 | loss 0.6894 | val_acc 0.5756\n",
      "ep 443 | loss 0.6894\n",
      "ep 444 | loss 0.6894 | val_acc 0.5756\n",
      "ep 445 | loss 0.6894\n",
      "ep 446 | loss 0.6894 | val_acc 0.5756\n",
      "ep 447 | loss 0.6894\n",
      "ep 448 | loss 0.6894 | val_acc 0.5756\n",
      "ep 449 | loss 0.6894\n",
      "ep 450 | loss 0.6894 | val_acc 0.5756\n",
      "ep 451 | loss 0.6894\n",
      "ep 452 | loss 0.6894 | val_acc 0.5756\n",
      "ep 453 | loss 0.6894\n",
      "ep 454 | loss 0.6894 | val_acc 0.5756\n",
      "ep 455 | loss 0.6894\n",
      "ep 456 | loss 0.6894 | val_acc 0.5756\n",
      "ep 457 | loss 0.6894\n",
      "ep 458 | loss 0.6894 | val_acc 0.5756\n",
      "ep 459 | loss 0.6894\n",
      "ep 460 | loss 0.6894 | val_acc 0.5756\n",
      "ep 461 | loss 0.6894\n",
      "ep 462 | loss 0.6894 | val_acc 0.5756\n",
      "ep 463 | loss 0.6894\n",
      "ep 464 | loss 0.6894 | val_acc 0.5756\n",
      "ep 465 | loss 0.6894\n",
      "ep 466 | loss 0.6894 | val_acc 0.5756\n",
      "ep 467 | loss 0.6894\n",
      "ep 468 | loss 0.6894 | val_acc 0.5756\n",
      "ep 469 | loss 0.6894\n",
      "ep 470 | loss 0.6894 | val_acc 0.5756\n",
      "ep 471 | loss 0.6894\n",
      "ep 472 | loss 0.6894 | val_acc 0.5756\n",
      "ep 473 | loss 0.6894\n",
      "ep 474 | loss 0.6894 | val_acc 0.5756\n",
      "ep 475 | loss 0.6894\n",
      "ep 476 | loss 0.6894 | val_acc 0.5756\n",
      "ep 477 | loss 0.6894\n",
      "ep 478 | loss 0.6894 | val_acc 0.5756\n",
      "ep 479 | loss 0.6894\n",
      "ep 480 | loss 0.6894 | val_acc 0.5756\n",
      "ep 481 | loss 0.6894\n",
      "ep 482 | loss 0.6894 | val_acc 0.5756\n",
      "ep 483 | loss 0.6894\n",
      "ep 484 | loss 0.6894 | val_acc 0.5756\n",
      "ep 485 | loss 0.6894\n",
      "ep 486 | loss 0.6894 | val_acc 0.5756\n",
      "ep 487 | loss 0.6894\n",
      "ep 488 | loss 0.6894 | val_acc 0.5756\n",
      "ep 489 | loss 0.6894\n",
      "ep 490 | loss 0.6894 | val_acc 0.5756\n",
      "ep 491 | loss 0.6894\n",
      "ep 492 | loss 0.6894 | val_acc 0.5756\n",
      "ep 493 | loss 0.6894\n",
      "ep 494 | loss 0.6894 | val_acc 0.5756\n",
      "ep 495 | loss 0.6894\n",
      "ep 496 | loss 0.6894 | val_acc 0.5756\n",
      "ep 497 | loss 0.6894\n",
      "ep 498 | loss 0.6894 | val_acc 0.5756\n",
      "ep 499 | loss 0.6894\n",
      "ep 500 | loss 0.6894 | val_acc 0.5756\n",
      "test_acc 0.4879\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"bow\":\n",
    "    Xtr = make_hash_X(train[\"text\"].tolist(), bins=50000)\n",
    "    Xva = make_hash_X(val[\"text\"].tolist(), bins=50000)\n",
    "    Xte = make_hash_X(test[\"text\"].tolist(), bins=50000)\n",
    "\n",
    "    model = train_lr(Xtr, ytr, Xva, yva, epochs=50, lr=0.2, eval_every=1)\n",
    "    print(f\"test_acc {eval_acc(model, Xte, yte):.4f}\")\n",
    "\n",
    "elif MODE == \"finbert\":\n",
    "    FINBERT_MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "    all_f = get_finbert_daily(day[[\"date\", \"title_list\"]], FINBERT_MODEL, bs=16, max_len=64)\n",
    "\n",
    "    ftr = train[[\"date\"]].merge(all_f, on=\"date\", how=\"left\")\n",
    "    fva = val[[\"date\"]].merge(all_f, on=\"date\", how=\"left\")\n",
    "    fte = test[[\"date\"]].merge(all_f, on=\"date\", how=\"left\")\n",
    "\n",
    "    Xtr = torch.tensor(ftr[[\"p_neg\", \"p_neu\", \"p_pos\", \"n_headlines\"]].values, dtype=torch.float32)\n",
    "    Xva = torch.tensor(fva[[\"p_neg\", \"p_neu\", \"p_pos\", \"n_headlines\"]].values, dtype=torch.float32)\n",
    "    Xte = torch.tensor(fte[[\"p_neg\", \"p_neu\", \"p_pos\", \"n_headlines\"]].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    model = train_lr(Xtr, ytr, Xva, yva, epochs=500, lr=0.5, eval_every=2)\n",
    "    print(f\"test_acc {eval_acc(model, Xte, yte):.4f}\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Mistakes were made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e6838",
   "metadata": {},
   "source": [
    "naive bayes experiment, not sure if i have the time to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbf2cc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_val_acc 0.5382\n",
      "nb_test_acc 0.4991\n"
     ]
    }
   ],
   "source": [
    "#naive bayes experiment\n",
    "\n",
    "def _nb_make_hash_counts(texts, bins=50000):\n",
    "    X = np.zeros((len(texts), bins), dtype=np.float32)\n",
    "    for i, t in enumerate(texts):\n",
    "        for w in toks(t):\n",
    "            X[i, stable_hash(w, bins)] += 1.0\n",
    "    return X\n",
    "\n",
    "def _nb_fit_multinomial(X, y, alpha=1.0):\n",
    "    y = np.asarray(y).astype(np.int64)\n",
    "    n, d = X.shape\n",
    "\n",
    "    n1 = int(y.sum())\n",
    "    n0 = n - n1\n",
    "\n",
    "    logp0 = np.log((n0 + 1e-12) / n)\n",
    "    logp1 = np.log((n1 + 1e-12) / n)\n",
    "\n",
    "\n",
    "    X0 = X[y == 0].sum(axis=0) + alpha\n",
    "    X1 = X[y == 1].sum(axis=0) + alpha\n",
    "\n",
    "    logphi0 = np.log(X0 / X0.sum())\n",
    "    logphi1 = np.log(X1 / X1.sum())\n",
    "\n",
    "    return {\"logp0\": logp0, \"logp1\": logp1, \"logphi0\": logphi0, \"logphi1\": logphi1}\n",
    "\n",
    "def _nb_predict_p1(m, X):\n",
    "    s0 = m[\"logp0\"] + X @ m[\"logphi0\"]\n",
    "    s1 = m[\"logp1\"] + X @ m[\"logphi1\"]\n",
    "    mx = np.maximum(s0, s1)\n",
    "    p1 = np.exp(s1 - mx) / (np.exp(s0 - mx) + np.exp(s1 - mx))\n",
    "    return p1\n",
    "\n",
    "def _nb_acc(m, X, y, name=\"test\"):\n",
    "    y = np.asarray(y).astype(np.float32)\n",
    "    p1 = _nb_predict_p1(m, X)\n",
    "    pred = (p1 >= 0.5).astype(np.float32)\n",
    "    acc = (pred == y).mean()\n",
    "    print(f\"nb_{name}_acc {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    #\n",
    "    Xtr_nb = _nb_make_hash_counts(train[\"text\"].tolist(), bins=50000)\n",
    "    Xva_nb = _nb_make_hash_counts(val[\"text\"].tolist(), bins=50000)\n",
    "    Xte_nb = _nb_make_hash_counts(test[\"text\"].tolist(), bins=50000)\n",
    "\n",
    "    nb_model = _nb_fit_multinomial(Xtr_nb, train[\"y\"].values, alpha=1.0)\n",
    "    _nb_acc(nb_model, Xva_nb, val[\"y\"].values, name=\"val\")\n",
    "    _nb_acc(nb_model, Xte_nb, test[\"y\"].values, name=\"test\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"Mistakes were made again\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
